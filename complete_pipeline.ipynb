{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Copyright 2023 Enzo Alexander Cording - https://github.com/EnzoCording - GNU GPL v3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline walks through the entire functionalities of FleetRL\n",
    "\n",
    "1) Creating a custom use-case\n",
    "2) Training an RL agent\n",
    "3) Building benchmark charging strategies\n",
    "4) Comparing the RL agent to the benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code could also be run in a .py file. Then, the code after the imports should be wrapped in:\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        #code here\n",
    "\n",
    "to allow for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T07:44:15.800322Z",
     "end_time": "2024-03-05T07:44:18.703727Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from fleetrl.benchmarking.linear_optimization import LinearOptimization\n",
    "from fleetrl.fleet_env.fleet_environment import FleetEnv\n",
    "from fleetrl.benchmarking.benchmark import Benchmark\n",
    "from fleetrl.benchmarking.uncontrolled_charging import Uncontrolled\n",
    "from fleetrl.agent_eval.evaluation import Evaluation\n",
    "from fleetrl.agent_eval.basic_evaluation import BasicEvaluation\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, ProgressBarCallback, BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "\n",
    "from pink import PinkActionNoise\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise, NormalActionNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a custom use-case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General settings**\n",
    "Under general settings, you can adjust how many vehicles to optimize for, whether you would like to create new schedules how long the episodes should be, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T07:44:18.708127Z",
     "end_time": "2024-03-05T07:44:18.710394Z"
    }
   },
   "outputs": [],
   "source": [
    "# define fundamental parameters\n",
    "run_name = \"LMD_2022_arbitrage_PPO_test\"\n",
    "n_train_steps = 48  # number of hours in a training episode\n",
    "n_eval_steps = 48  # number of hours in one evaluation episode\n",
    "n_eval_episodes = 1  # number of episodes for evaluation\n",
    "n_evs = 2  # number of evs\n",
    "n_envs = 2  # number of envs parallel - has to be equal to 1, if train_freq = (1, episode) or default setting\n",
    "time_steps_per_hour = 4  # temporal resolution\n",
    "use_case: str = \"lmd\"  # for file name\n",
    "scenario: Literal[\"arb\", \"tariff\"] = \"tariff\"  # arbitrage or tariff\n",
    "gen_new_schedule = False  # generate a new schedule - refer to schedule generator and its config to change settings\n",
    "gen_new_test_schedule = False  # generate a new schedule for agent testing\n",
    "real_time = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File saving**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training settings**\n",
    "These more low-level settings allow you to change training-related parameters. Refer to the documentation of FleetRL and stable-baselines3 for further details. Observations are by default normalized within SB3, due to their rolling average normalization. You can also conduct absolute normalization via FleetRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T07:44:18.713738Z",
     "end_time": "2024-03-05T07:44:18.722209Z"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "norm_obs_in_env = False  # normalize observations in FleetRL (max, min normalization)\n",
    "vec_norm_obs = True  # normalize observations in SB3 (rolling normalization)\n",
    "vec_norm_rew = True  # normalize rewards in SB3 (rolling normalization)\n",
    "total_steps = int(1e3)  # total training time steps\n",
    "saving_interval = 5e2  # interval for saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for environment object creation**\n",
    "Further settings can be adjusted below, view the comments and docs for more detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T07:44:18.730194Z",
     "end_time": "2024-03-05T07:44:18.761924Z"
    }
   },
   "outputs": [],
   "source": [
    "# environment arguments - adjust settings if necessary\n",
    "# additional settings can be changed in the config files\n",
    "env_kwargs = {\"schedule_name\": str(n_evs) + \"_\" + str(use_case) + \".csv\",\n",
    "              \"building_name\": \"load_\" + str(use_case) + \".csv\",\n",
    "              \"use_case\": use_case,\n",
    "              \"include_building\": True,  # False removes building load from Observation\n",
    "              \"include_pv\": True,  # False removes PV from Observation\n",
    "              \"time_picker\": \"random\",  # Pick a random starting day in the schedule dataframe\n",
    "              \"deg_emp\": False,  # empirical degradation calculation\n",
    "              \"include_price\": True,  # False removes electricity prices from Observation\n",
    "              \"ignore_price_reward\": False,  # True sets price-related reward coefficient to 0\n",
    "              \"ignore_invalid_penalty\": False,  # True ignores penalties on invalid actions (charging an empty spot)\n",
    "              \"ignore_overcharging_penalty\": False,  # True ignores penalties on charging signals above target SOC\n",
    "              \"ignore_overloading_penalty\": False,  # True ignores grid connection overloading penalty\n",
    "              \"episode_length\": n_train_steps,  # in hours\n",
    "              \"normalize_in_env\": norm_obs_in_env,  # Conduct normalization within FleetRL\n",
    "              \"verbose\": 0,  # Print statements, can slow down FPS\n",
    "              \"aux\": True,  # Include auxiliary data (recommended)\n",
    "              \"log_data\": False,  # Log data (Makes most sense for evaluation runs)\n",
    "              \"calculate_degradation\": True,  # Calculate SOH degradation (Can slow down FPS)\n",
    "              \"target_soc\": 0.85,  # Signals that would charge above target SOC are clipped\n",
    "              \"gen_schedule\": gen_new_schedule,  # generate a new schedule\n",
    "              \"gen_start_date\": \"2022-01-01 00:00\",  # if new schedule, start date\n",
    "              \"gen_end_date\": \"2022-12-31 23:59:59\",  # if new schedule, end date\n",
    "              \"gen_name\": \"my_sched.csv\",  # name of new schedule\n",
    "              \"gen_n_evs\": 1,  # number of EVs in new sched, per EV it takes ca. 10-20 min.\n",
    "              \"seed\": 42,  # Seed for RNG\n",
    "              \"real_time\": real_time\n",
    "              }\n",
    "\n",
    "# commercial tariff scenario, fixed fee on spot price (+10 ct/kWh, and a 50% mark-up)\n",
    "# Feed-in tariff orientates after PV feed-in, with 25% deduction\n",
    "if scenario == \"tariff\":\n",
    "    env_kwargs[\"spot_markup\"] = 10\n",
    "    env_kwargs[\"spot_mul\"] = 1.5\n",
    "    env_kwargs[\"feed_in_ded\"] = 0.25\n",
    "    env_kwargs[\"price_name\"] = \"spot_2021_new.csv\"\n",
    "    env_kwargs[\"tariff_name\"] = \"fixed_feed_in.csv\"\n",
    "\n",
    "# arbitrage scenario, up and down prices are spot price, no markups or taxes\n",
    "elif scenario == \"arb\":\n",
    "    env_kwargs[\"spot_markup\"] = 0\n",
    "    env_kwargs[\"spot_mul\"] = 1\n",
    "    env_kwargs[\"feed_in_ded\"] = 0\n",
    "    env_kwargs[\"price_name\"] = \"spot_2021_new.csv\"\n",
    "    env_kwargs[\"tariff_name\"] = \"spot_2021_new_tariff.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment object creation**\n",
    "Vec_Env are created to enable multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:17.295890Z",
     "end_time": "2024-03-04T08:07:24.434691Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vec_env = make_vec_env(FleetEnv,\n",
    "                             n_envs=n_envs,\n",
    "                             vec_env_cls=SubprocVecEnv,\n",
    "                             env_kwargs=env_kwargs,\n",
    "                             seed=env_kwargs[\"seed\"])\n",
    "\n",
    "train_norm_vec_env = VecNormalize(venv=train_vec_env,\n",
    "                                  norm_obs=vec_norm_obs,\n",
    "                                  norm_reward=vec_norm_rew,\n",
    "                                  training=True,\n",
    "                                  clip_reward=10.0)\n",
    "\n",
    "env_kwargs[\"time_picker\"] = \"eval\"\n",
    "\n",
    "if gen_new_schedule:\n",
    "    env_kwargs[\"gen_schedule\"] = False\n",
    "    env_kwargs[\"schedule_name\"] = env_kwargs[\"gen_name\"]\n",
    "\n",
    "eval_vec_env = make_vec_env(FleetEnv,\n",
    "                             n_envs=n_envs,\n",
    "                             vec_env_cls=SubprocVecEnv,\n",
    "                             env_kwargs=env_kwargs,\n",
    "                             seed=env_kwargs[\"seed\"])\n",
    "\n",
    "eval_norm_vec_env = VecNormalize(venv=eval_vec_env,\n",
    "                                  norm_obs=vec_norm_obs,\n",
    "                                  norm_reward=vec_norm_rew,\n",
    "                                  training=True,\n",
    "                                  clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a schedule for testing the trained agents on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:24.446091Z",
     "end_time": "2024-03-04T08:07:27.957495Z"
    }
   },
   "outputs": [],
   "source": [
    "if gen_new_test_schedule:\n",
    "    # generate an evaluation schedule\n",
    "    test_sched_name = env_kwargs[\"gen_name\"]\n",
    "    if not test_sched_name.endswith(\".csv\"):\n",
    "        test_sched_name = test_sched_name + \"_test\" + \".csv\"\n",
    "    else:\n",
    "        test_sched_name = test_sched_name.strip(\".csv\")\n",
    "        test_sched_name = test_sched_name + \"_test\" + \".csv\"\n",
    "\n",
    "    env_kwargs[\"gen_schedule\"] = True\n",
    "    env_kwargs[\"gen_name\"] = test_sched_name\n",
    "\n",
    "    test_vec_env = make_vec_env(FleetEnv,\n",
    "                                n_envs=1,\n",
    "                                vec_env_cls=SubprocVecEnv,\n",
    "                                env_kwargs=env_kwargs,\n",
    "                                seed=env_kwargs[\"seed\"])\n",
    "\n",
    "    env_kwargs[\"gen_schedule\"] = False\n",
    "    env_kwargs[\"schedule_name\"] = test_sched_name\n",
    "\n",
    "test_vec_env = make_vec_env(FleetEnv,\n",
    "                            n_envs=n_envs,\n",
    "                            vec_env_cls=SubprocVecEnv,\n",
    "                            env_kwargs=env_kwargs,\n",
    "                            seed=env_kwargs[\"seed\"])\n",
    "\n",
    "test_norm_vec_env = VecNormalize(venv=test_vec_env,\n",
    "                                 norm_obs=vec_norm_obs,\n",
    "                                 norm_reward=vec_norm_rew,\n",
    "                                 training=True,\n",
    "                                 clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are regularly called during training and enable useful functionalities such as logging or progress reporting. View SB3 docs for further information. Note that wandb callbacks are possible with SB3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:27.964205Z",
     "end_time": "2024-03-04T08:07:27.967142Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env=eval_norm_vec_env,\n",
    "                             warn=True,\n",
    "                             verbose=1,\n",
    "                             deterministic=True,\n",
    "                             eval_freq=max(10000 // n_envs, 1),\n",
    "                             n_eval_episodes=5,\n",
    "                             render=False,\n",
    "                             )\n",
    "\n",
    "class HyperParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves hyperparameters and metrics at start of training, logging to tensorboard\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "\n",
    "        metric_dict = {\n",
    "            \"rollout/ep_len_mean\": 0,\n",
    "            \"train/value_loss\": 0.0,\n",
    "        }\n",
    "\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\")\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "progress_bar = ProgressBarCallback()\n",
    "\n",
    "## wandb callback possible, check documentation of SB3 and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:27.968684Z",
     "end_time": "2024-03-04T08:07:27.974412Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameter_callback = HyperParamCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:27.979825Z",
     "end_time": "2024-03-04T08:07:28.022209Z"
    }
   },
   "outputs": [],
   "source": [
    "# model-related settings\n",
    "n_actions = train_norm_vec_env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "noise_scale = 0.1\n",
    "seq_len = n_train_steps * time_steps_per_hour\n",
    "action_noise = PinkActionNoise(noise_scale, seq_len, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = PPO(policy=\"MlpPolicy\",\n",
    "            verbose=0, # setting verbose to 0 can introduce performance increases in jupyterlab environments\n",
    "            env=train_norm_vec_env,\n",
    "            tensorboard_log=\"./fleetrl/rl_agents/trained_agents/tb_log\")\n",
    "\n",
    "# might introduce performance increases\n",
    "            # gamma=0.99,\n",
    "            # learning_rate=0.0005,\n",
    "            # batch_size=128,\n",
    "            # n_epochs=8,\n",
    "            # gae_lambda=0.9,\n",
    "            # clip_range=0.2,\n",
    "            # clip_range_vf=None,\n",
    "            # normalize_advantage=True,\n",
    "            # ent_coef=0.0008,\n",
    "            # vf_coef=0.5,\n",
    "            # max_grad_norm=0.5,\n",
    "            # n_steps=2048)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T08:07:27.988460Z",
     "end_time": "2024-03-04T08:07:28.065793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./FleetRL/RL_agents/trained_agents/tb_log --port 6006 --bind_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T08:08:17.138641Z",
     "end_time": "2024-03-04T08:08:19.658985Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T08:08:26.326427Z",
     "end_time": "2024-03-04T08:08:26.330940Z"
    }
   },
   "outputs": [],
   "source": [
    "comment = run_name\n",
    "time_now = int(time.time())\n",
    "trained_agents_dir = f\"./fleetrl/rl_agents/trained_agents/vec_PPO_{time_now}_{run_name}\"\n",
    "logs_dir = f\"{trained_agents_dir}/logs/\"\n",
    "\n",
    "if not os.path.exists(trained_agents_dir):\n",
    "    os.makedirs(trained_agents_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:36:39.924934Z",
     "start_time": "2024-01-12T20:34:20.713341Z"
    }
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "# models are saved in a specified interval: once with unique step identifiers\n",
    "# model and the normalization metrics are saved as well, overwriting the previous file every time\n",
    "for i in range(0, int(total_steps / saving_interval)):\n",
    "    model.learn(total_timesteps=saving_interval,\n",
    "                reset_num_timesteps=False,\n",
    "                tb_log_name=f\"PPO_{time_now}_{comment}\",\n",
    "                callback=[eval_callback, hyperparameter_callback, progress_bar])\n",
    "\n",
    "    model.save(f\"{trained_agents_dir}/{saving_interval * i}\")\n",
    "\n",
    "    # Don't forget to save the VecNormalize statistics when saving the agent\n",
    "    tmp_dir = f\"{trained_agents_dir}/tmp/\"\n",
    "    model_path = tmp_dir + f\"PPO-fleet_{comment}_{time_now}\"\n",
    "    model.save(model_path)\n",
    "    stats_path = os.path.join(tmp_dir, f\"vec_normalize-{comment}_{time_now}.pkl\")\n",
    "    train_norm_vec_env.save(stats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:36:47.401779Z",
     "start_time": "2024-01-12T20:36:47.360363Z"
    }
   },
   "outputs": [],
   "source": [
    "# environment arguments for evaluation\n",
    "env_kwargs[\"time_picker\"] = \"static\"  # Pick a random starting day in the schedule dataframe\n",
    "env_kwargs[\"log_data\"] = True,  # Log data (Makes most sense for evaluation runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:36:48.269268Z",
     "start_time": "2024-01-12T20:36:48.263774Z"
    }
   },
   "outputs": [],
   "source": [
    "eval: Evaluation = BasicEvaluation(n_steps=n_eval_steps,\n",
    "                                   n_evs=n_evs,\n",
    "                                   n_episodes=n_eval_episodes,\n",
    "                                   n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:36:58.791428Z",
     "start_time": "2024-01-12T20:36:50.013919Z"
    }
   },
   "outputs": [],
   "source": [
    "stats_path = stats_path\n",
    "model_path = model_path\n",
    "\n",
    "rl_log = eval.evaluate_agent(env_kwargs=env_kwargs, norm_stats_path=stats_path, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:37:09.278233Z",
     "start_time": "2024-01-12T20:37:00.386528Z"
    }
   },
   "outputs": [],
   "source": [
    "uncontrolled_charging: Benchmark = Uncontrolled(n_steps=n_eval_steps,\n",
    "                                                n_evs=n_evs,\n",
    "                                                n_episodes=n_eval_episodes,\n",
    "                                                n_envs=1)\n",
    "\n",
    "uc_log = uncontrolled_charging.run_benchmark(env_kwargs=env_kwargs, use_case=use_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To try out linear optimisation, glpk must be installed. Alternatively, you can use your gurobi license. Simply swap out \"glpk\" for \"gurobi\" in linear_optimization.py in line 224"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lp: Benchmark = LinearOptimization(n_steps=n_eval_steps, n_evs=n_evs, n_episodes=n_eval_episodes, n_envs=1, time_steps_per_hour=time_steps_per_hour)\n",
    "\n",
    "lp_log = lp.run_benchmark(env_kwargs=env_kwargs, use_case=use_case, seed=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lp.plot_benchmark(lp_log)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:37:10.990547Z",
     "start_time": "2024-01-12T20:37:10.352784Z"
    }
   },
   "outputs": [],
   "source": [
    "uncontrolled_charging.plot_benchmark(uc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T20:37:13.853321Z",
     "start_time": "2024-01-12T20:37:12.225344Z"
    }
   },
   "outputs": [],
   "source": [
    "eval.compare(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.plot_soh(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.plot_violations(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.plot_action_dist(rl_log=rl_log, benchmark_log=uc_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
