{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:27.449177Z",
     "start_time": "2023-06-18T22:46:25.040277Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import FleetRL\n",
    "from FleetRL.fleet_env.fleet_environment import FleetEnv\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, ProgressBarCallback, BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "\n",
    "from pink import PinkNoiseDist, PinkActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T21:16:43.248490Z",
     "start_time": "2023-06-18T21:16:43.242601Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = \"LMD_2021_realistic_PPO_mul3\"\n",
    "comment = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T21:16:43.298801Z",
     "start_time": "2023-06-18T21:16:43.255700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_now = int(time.time())\n",
    "trained_agents_dir = f\"./trained/vec_PPO-{time_now}-{run_name}\"\n",
    "logs_dir = f\"./logs/vec_PPO-{time_now}-{run_name}\"\n",
    "\n",
    "if not os.path.exists(trained_agents_dir):\n",
    "    os.makedirs(trained_agents_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:35.849875Z",
     "start_time": "2023-06-18T22:46:29.818869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_args = {\"schedule_name\": \"lmd_sched_single.csv\",\n",
    "            \"building_name\": \"load_lmd.csv\",\n",
    "            \"price_name\": \"spot_2021_new.csv\",\n",
    "            \"tariff_name\": \"fixed_feed_in.csv\",\n",
    "            \"use_case\": \"lmd\",\n",
    "            \"verbose\": False,\n",
    "            \"time_picker\": \"random\",\n",
    "            \"episode_length\": 48,\n",
    "            \"calculate_degradation\": True,\n",
    "            \"log_data\": False,\n",
    "            \"normalize_in_env\": False,\n",
    "            \"aux\": True,\n",
    "            \"spot_markup\": 10,\n",
    "            \"spot_mul\": 1.5,\n",
    "            \"feed_in_ded\": 0.25\n",
    "            }\n",
    "\n",
    "n_cpu = 10\n",
    "train_vec_env = make_vec_env(FleetEnv,\n",
    "                             seed=3,\n",
    "                             n_envs=n_cpu,\n",
    "                             vec_env_cls=SubprocVecEnv,\n",
    "                             env_kwargs=env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:35.854925Z",
     "start_time": "2023-06-18T22:46:35.853414Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_norm_vec_env = VecNormalize(venv=train_vec_env, norm_obs=True, norm_reward=True, training=True, clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:39.369517Z",
     "start_time": "2023-06-18T22:46:35.898496Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_args = {\"schedule_name\": \"lmd_sched_single.csv\",\n",
    "            \"building_name\": \"load_lmd.csv\",\n",
    "            \"price_name\": \"spot_2021_new.csv\",\n",
    "            \"tariff_name\": \"fixed_feed_in.csv\",\n",
    "            \"use_case\": \"lmd\",\n",
    "            \"verbose\": False,\n",
    "            \"time_picker\": \"eval\",\n",
    "            \"episode_length\": 48,\n",
    "            \"calculate_degradation\": True,\n",
    "            \"log_data\": False,\n",
    "            \"normalize_in_env\": False,\n",
    "            \"aux\": True,\n",
    "            \"spot_markup\": 10,\n",
    "            \"spot_mul\": 1.5,\n",
    "            \"feed_in_ded\": 0.25\n",
    "            }\n",
    "\n",
    "eval_vec_env = make_vec_env(FleetEnv,\n",
    "                            seed=3,\n",
    "                            n_envs=1,\n",
    "                            vec_env_cls=SubprocVecEnv,\n",
    "                            env_kwargs=env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:39.376182Z",
     "start_time": "2023-06-18T22:46:39.371022Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_norm_vec_env = VecNormalize(venv=eval_vec_env,\n",
    "                                 norm_obs=True,\n",
    "                                 norm_reward=True,\n",
    "                                 training=True,\n",
    "                                 clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:59.480927Z",
     "start_time": "2023-06-18T22:46:59.475386Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env=eval_norm_vec_env,\n",
    "                             warn=True,\n",
    "                             verbose=1,\n",
    "                             deterministic=True,\n",
    "                             eval_freq=max(10000 // n_cpu, 1),\n",
    "                             n_eval_episodes=5,\n",
    "                             render=False,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperParamCallback(BaseCallback):\n",
    "\n",
    "    \"\"\"\n",
    "    Saves hyperparameters and metrics at start of training, logging to tensorboard\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "            \"tau\": self.model.tau,\n",
    "            \"learning starts\": self.model.learning_starts,\n",
    "            \"batch size\": self.model.batch_size,\n",
    "            \"buffer size\": self.model.buffer_size,\n",
    "            \"policy_delay\": self.model.policy_delay,\n",
    "        }\n",
    "\n",
    "        metric_dict = {\n",
    "            \"rollout/ep_len_mean\": 0,\n",
    "            \"train/value_loss\": 0.0,\n",
    "        }\n",
    "\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\")\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameter_callback = HyperParamCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:59.843009Z",
     "start_time": "2023-06-18T22:46:59.832197Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_actions = train_norm_vec_env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "noise_scale = 0.1\n",
    "seq_len = 48 * 4\n",
    "action_noise = PinkActionNoise(noise_scale, seq_len, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:47:00.416896Z",
     "start_time": "2023-06-18T22:47:00.378841Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "model = PPO(policy=\"MlpPolicy\",\n",
    "            env=train_norm_vec_env,\n",
    "            verbose=0,\n",
    "            train_freq=(4, \"step\"),\n",
    "            learning_rate=0.001,\n",
    "            learning_starts=20000,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "            buffer_size=500000,\n",
    "            tau=0.01,\n",
    "            tensorboard_log=\"./tb_log\",\n",
    "            action_noise=action_noise\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = PPO(policy=\"MlpPolicy\",\n",
    "           verbose=0,\n",
    "           env = train_norm_vec_env,\n",
    "           tensorboard_log = \"./tb_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_vec_env.load(\"./tmp/vec_PPO/vec_normalize-LMD_2021_realistic_PPO_mul3.pkl\", train_norm_vec_env)\n",
    "model = PPO.load(\"./tmp/vec_PPO/PPO-fleet_LMD_2021_realistic_PPO_mul3.zip\", env = train_norm_vec_env,\n",
    "                custom_objects={\"observation_space\": train_norm_vec_env.observation_space,\n",
    "                                \"action_space\": train_norm_vec_env.action_space})\n",
    "#model.ent_coef = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T17:56:36.392347Z",
     "iopub.status.busy": "2023-07-08T17:56:36.391665Z",
     "iopub.status.idle": "2023-07-08T17:56:39.050837Z",
     "shell.execute_reply": "2023-07-08T17:56:39.049872Z",
     "shell.execute_reply.started": "2023-07-08T17:56:36.392309Z"
    }
   },
   "source": [
    "train_norm_vec_env.load(\"./tmp/vec_PPO/vec_normalize-LMD_2021_realistic_PPO_mul3.pkl\", train_norm_vec_env)\n",
    "model = PPO.load(\"./tmp/vec_PPO/PPO-fleet_LMD_2021_realistic_PPO.zi\", env = train_norm_vec_env)\n",
    "model.gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T22:46:41.189642Z",
     "start_time": "2023-06-18T22:46:40.190292Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5744400, episode_reward=-21.32 +/- 31.87\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5754400, episode_reward=-69.39 +/- 46.37\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5764400, episode_reward=-31.21 +/- 31.16\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5774400, episode_reward=-36.11 +/- 32.39\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5784400, episode_reward=-35.35 +/- 57.54\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5794400, episode_reward=-32.60 +/- 32.53\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5804400, episode_reward=-17.73 +/- 13.21\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5814400, episode_reward=-41.35 +/- 35.82\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5824400, episode_reward=-6.80 +/- 15.08\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5834400, episode_reward=-43.45 +/- 32.56\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5844400, episode_reward=-21.48 +/- 20.43\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5854400, episode_reward=-20.67 +/- 27.56\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5864400, episode_reward=-32.84 +/- 25.16\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5874400, episode_reward=-10.64 +/- 3.04\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5884400, episode_reward=-8.39 +/- 22.02\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5894400, episode_reward=-29.80 +/- 25.42\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5904400, episode_reward=-13.16 +/- 3.14\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5914400, episode_reward=-33.34 +/- 30.74\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5924400, episode_reward=-3.19 +/- 14.28\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5934400, episode_reward=-39.41 +/- 31.44\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5944400, episode_reward=-52.67 +/- 28.90\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5954400, episode_reward=-20.94 +/- 36.25\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5964400, episode_reward=-24.88 +/- 17.82\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5974400, episode_reward=-29.54 +/- 26.33\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5984400, episode_reward=-59.85 +/- 81.99\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=5994400, episode_reward=-25.21 +/- 15.21\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6004400, episode_reward=-34.36 +/- 33.08\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6014400, episode_reward=-58.98 +/- 62.12\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6024400, episode_reward=-26.38 +/- 6.37\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6034400, episode_reward=-17.90 +/- 17.26\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6044400, episode_reward=-19.19 +/- 27.60\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6054400, episode_reward=-31.12 +/- 26.54\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6064400, episode_reward=-23.90 +/- 14.74\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6074400, episode_reward=-27.30 +/- 23.07\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6084400, episode_reward=-17.87 +/- 4.28\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6094400, episode_reward=-21.95 +/- 24.62\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6104400, episode_reward=-21.49 +/- 23.13\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6114400, episode_reward=-19.05 +/- 16.94\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6124400, episode_reward=-25.35 +/- 21.49\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6134400, episode_reward=-27.35 +/- 18.60\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6144400, episode_reward=-5.08 +/- 13.72\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6154400, episode_reward=-11.34 +/- 11.47\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6164400, episode_reward=-30.34 +/- 21.45\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6174400, episode_reward=0.11 +/- 10.94\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6184400, episode_reward=-23.68 +/- 12.12\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6194400, episode_reward=-8.20 +/- 11.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6204400, episode_reward=-29.30 +/- 18.80\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6214400, episode_reward=-22.26 +/- 14.70\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6224400, episode_reward=-33.07 +/- 22.07\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6234400, episode_reward=-14.73 +/- 24.56\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6244400, episode_reward=-21.32 +/- 24.75\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6254400, episode_reward=-26.92 +/- 34.52\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6264400, episode_reward=-24.61 +/- 20.76\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6274400, episode_reward=-40.76 +/- 18.19\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6284400, episode_reward=-27.46 +/- 26.21\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6294400, episode_reward=-20.37 +/- 31.17\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6304400, episode_reward=-18.25 +/- 18.27\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6314400, episode_reward=-21.00 +/- 17.52\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6324400, episode_reward=-26.18 +/- 17.00\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6334400, episode_reward=-2.23 +/- 9.21\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6344400, episode_reward=-32.79 +/- 13.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6354400, episode_reward=-11.74 +/- 23.04\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6364400, episode_reward=-19.10 +/- 23.82\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6374400, episode_reward=-23.68 +/- 24.31\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6384400, episode_reward=-8.86 +/- 25.53\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6394400, episode_reward=-19.45 +/- 23.15\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6404400, episode_reward=-37.43 +/- 33.37\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6414400, episode_reward=-16.07 +/- 17.85\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6424400, episode_reward=-33.47 +/- 43.14\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6434400, episode_reward=0.97 +/- 11.77\n",
      "Episode length: 192.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6444400, episode_reward=-14.87 +/- 35.44\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6454400, episode_reward=-33.28 +/- 26.77\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6464400, episode_reward=-28.02 +/- 24.86\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6474400, episode_reward=-3.02 +/- 11.50\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6484400, episode_reward=-39.64 +/- 47.49\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6494400, episode_reward=-53.10 +/- 42.50\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6504400, episode_reward=-16.19 +/- 14.45\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6514400, episode_reward=-19.62 +/- 18.16\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6524400, episode_reward=-19.43 +/- 13.87\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6534400, episode_reward=-45.11 +/- 47.81\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6544400, episode_reward=-14.10 +/- 19.54\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6554400, episode_reward=-16.73 +/- 29.48\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6564400, episode_reward=-11.71 +/- 21.87\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6574400, episode_reward=-31.28 +/- 17.10\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6584400, episode_reward=-20.25 +/- 16.80\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6594400, episode_reward=-20.32 +/- 22.94\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6604400, episode_reward=-24.74 +/- 16.67\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6614400, episode_reward=-17.54 +/- 19.22\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6624400, episode_reward=-14.63 +/- 4.89\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6634400, episode_reward=-30.81 +/- 33.67\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6644400, episode_reward=-10.39 +/- 12.67\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6654400, episode_reward=-1.87 +/- 14.47\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6664400, episode_reward=-4.83 +/- 12.17\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6674400, episode_reward=-25.83 +/- 29.52\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6684400, episode_reward=-39.36 +/- 25.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6694400, episode_reward=-23.88 +/- 32.43\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6704400, episode_reward=-18.93 +/- 25.88\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6714400, episode_reward=-27.21 +/- 36.10\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6724400, episode_reward=-36.58 +/- 33.80\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6734400, episode_reward=-19.96 +/- 23.09\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6744400, episode_reward=-26.47 +/- 21.66\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6754400, episode_reward=-7.81 +/- 18.58\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6764400, episode_reward=-24.37 +/- 31.96\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6774400, episode_reward=-19.98 +/- 18.71\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6784400, episode_reward=-26.39 +/- 22.50\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6794400, episode_reward=-16.73 +/- 25.99\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6804400, episode_reward=-25.53 +/- 12.95\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6814400, episode_reward=-35.42 +/- 19.07\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6824400, episode_reward=-18.39 +/- 10.75\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6834400, episode_reward=-22.60 +/- 14.60\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6844400, episode_reward=-35.73 +/- 56.99\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6854400, episode_reward=-9.01 +/- 12.89\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6864400, episode_reward=-34.41 +/- 10.16\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6874400, episode_reward=-2.12 +/- 13.44\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6884400, episode_reward=-25.70 +/- 20.02\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6894400, episode_reward=-18.08 +/- 13.14\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6904400, episode_reward=-17.32 +/- 24.45\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6914400, episode_reward=-14.98 +/- 7.18\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6924400, episode_reward=-28.76 +/- 57.15\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6934400, episode_reward=-12.06 +/- 21.82\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6944400, episode_reward=-33.09 +/- 53.54\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6954400, episode_reward=-16.23 +/- 23.70\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6964400, episode_reward=-51.01 +/- 53.11\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6974400, episode_reward=-60.84 +/- 49.41\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6984400, episode_reward=-17.21 +/- 17.66\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=6994400, episode_reward=-59.07 +/- 90.27\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7004400, episode_reward=-17.27 +/- 12.43\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7014400, episode_reward=-40.67 +/- 48.81\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7024400, episode_reward=-50.83 +/- 51.21\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7034400, episode_reward=-44.15 +/- 47.03\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7044400, episode_reward=-14.83 +/- 16.43\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7054400, episode_reward=-13.93 +/- 20.66\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7064400, episode_reward=-18.83 +/- 13.22\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7074400, episode_reward=-18.56 +/- 13.61\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7084400, episode_reward=-1.85 +/- 11.28\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7094400, episode_reward=-52.52 +/- 76.18\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7104400, episode_reward=-31.30 +/- 42.57\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7114400, episode_reward=-2.78 +/- 11.95\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7124400, episode_reward=-47.58 +/- 37.77\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7134400, episode_reward=-26.82 +/- 27.29\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7144400, episode_reward=-22.38 +/- 37.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7154400, episode_reward=-6.07 +/- 12.43\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7164400, episode_reward=-29.93 +/- 12.09\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7174400, episode_reward=-40.06 +/- 31.06\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7184400, episode_reward=-10.30 +/- 12.06\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7194400, episode_reward=-24.51 +/- 20.39\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7204400, episode_reward=-5.18 +/- 15.41\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7214400, episode_reward=-0.58 +/- 5.17\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7224400, episode_reward=-19.62 +/- 24.49\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7234400, episode_reward=-17.97 +/- 18.14\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7244400, episode_reward=-46.23 +/- 81.47\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7254400, episode_reward=-59.22 +/- 80.01\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7264400, episode_reward=-26.65 +/- 23.73\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7274400, episode_reward=-26.90 +/- 19.02\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7284400, episode_reward=-16.07 +/- 21.93\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7294400, episode_reward=-38.25 +/- 25.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7304400, episode_reward=-3.22 +/- 4.33\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7314400, episode_reward=-24.51 +/- 21.47\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7324400, episode_reward=-20.12 +/- 20.42\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7334400, episode_reward=-29.76 +/- 18.15\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7344400, episode_reward=-7.67 +/- 10.64\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7354400, episode_reward=-21.62 +/- 14.98\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7364400, episode_reward=-40.79 +/- 61.85\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7374400, episode_reward=-16.47 +/- 23.50\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7384400, episode_reward=-17.25 +/- 23.77\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7394400, episode_reward=-34.34 +/- 27.94\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7404400, episode_reward=-13.96 +/- 14.14\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7414400, episode_reward=-54.77 +/- 65.78\n",
      "Episode length: 192.00 +/- 0.00\n",
      "Eval num_timesteps=7424400, episode_reward=-34.46 +/- 16.94\n",
      "Episode length: 192.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "saving_interval = 50000\n",
    "for i in range(0, 80):\n",
    "    model.learn(total_timesteps=saving_interval, \n",
    "                reset_num_timesteps=False, \n",
    "                tb_log_name=f\"PPO_{time_now}_{comment}\",\n",
    "                callback=[eval_callback])\n",
    "    \n",
    "    model.save(f\"{trained_agents_dir}/{saving_interval * i}\")\n",
    "\n",
    "    # Don't forget to save the VecNormalize statistics when saving the agent\n",
    "    log_dir = \"./tmp/vec_PPO/\"\n",
    "    model.save(log_dir + f\"PPO-fleet_{comment}\")\n",
    "    stats_path = os.path.join(log_dir, f\"vec_normalize-{comment}.pkl\")\n",
    "    train_norm_vec_env.save(stats_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
